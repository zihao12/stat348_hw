---
title: "hw2"
author: "zihao12"
date: "2021-04-10"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---


```{r message=FALSE, warning=FALSE}
rm(list = ls())
library(glmnet)
library(stats)
```


# Problem A

## i) Investigate `glmnet`
I ran the code in https://github.com/stephens999/stat34800/blob/master/analysis/glmnet_intro.Rmd . Below I did further investigations. 
```{r}
simulate_data1 <- function(p, n_train, n_test, n_sd = 25, seed = 123){
  set.seed(seed)
  n = n_train + n_test
  X = matrix(rnorm(n*p),ncol=p)
  b = rnorm(p)
  e = rnorm(n,0,sd=n_sd)
  Y = X %*% b + e
  return(list(b = b, 
              X_train = X[1:n_train,], X_test = X[(n_train + 1):n,], 
              y_train = Y[1:n_train], y_test = Y[(n_train + 1):n]))
}
```

### (a) compare CV results with test 
Simulate some independent test data from the same model and check that the prediction error of different methods is comparable with the CV results.
```{r}
data_sim <- simulate_data1(p = 100, n_train = 500, n_test = 500)

```


Ridge regression: 
```{r}
cv.ridge = cv.glmnet(data_sim$X_train,data_sim$y_train,alpha=0, nfolds = 10)
Y.ridge = glmnet(data_sim$X_train,data_sim$y_train,alpha=0)
Yhat_test = predict(Y.ridge, newx = data_sim$X_test, s = cv.ridge$lambda)
err_test = colMeans((Yhat_test - data_sim$y_test)^2)
ylim = c(min(cv.ridge$cvm - cv.ridge$cvsd), max(err_test))
plot(cv.ridge, ylim = ylim, main = "ridge regression prediction error: test vs CV")
lines(log(cv.ridge$lambda), err_test, col = "blue")
```

Lasso regression:
```{r}
cv.lasso = cv.glmnet(data_sim$X_train,data_sim$y_train,alpha=1, nfolds = 10)
Y.lasso = glmnet(data_sim$X_train,data_sim$y_train,alpha=1)
Yhat_test = predict(Y.lasso, newx = data_sim$X_test, s = cv.lasso$lambda)
err_test = colMeans((Yhat_test - data_sim$y_test)^2)
ylim = c(min(cv.lasso$cvm - cv.lasso$cvsd), max(err_test))
plot(cv.lasso, ylim = ylim, main = "lasso regression prediction error: test vs CV")
lines(log(cv.lasso$lambda), err_test, col = "blue")
```


#### Comment:
* The MSE from CV is biased (always underestimated); when setting `nfolds` smaller (didn't show above), the bias gets smaller. 
* However, we can find the appropriate $\lambda$ using CV (that also miniimizes prediction error)

### (b)
Plot the (non-intercept) coefficients obtained from ridge regression and lasso against the true values used in the simulation and discuss the "shrinkage" that is occurring.
```{r}
b.ridge = predict(Y.ridge, type="coefficients", s = cv.ridge$lambda.min)

b.lasso = predict(Y.lasso, type="coefficients", s = cv.lasso$lambda.min)


plot(c(0, data_sim$b), b.ridge, xlab = "b.true")
abline(a = 0, b = 1)

plot(c(0, data_sim$b), b.lasso, xlab = "b.true")
abline(a = 0, b = 1)
```

#### comment on shrinkage
* The ridge regression shrinks each $\hat{\beta}_j$ by multiplying a constant, which can be seen in the plot (rotating a little bit)
* The lasso regression does soft thresholding: setting those $\hat{\beta}_j$ smaller than a cutoff value to 0; this can also be seen in the plot. 

### (c) 
Plot the estimated (non-intercept) coefficients against the "theoretical" expectations you would expect if the predictors were orthogonal. Eg the "soft thresholding" property for the Lasso. (Note that the predictors here are not orthogonal, so the theory will certainly not hold precisely - does it hold approximately?)

* From the book we know, if $X$ has orthonormal columns, coefficients are ridge are $\hat{\beta}_{j} /(1+\lambda)$ and for lasso are $\operatorname{sign}\left(\hat{\beta}_{j}\right)\left(\left|\hat{\beta}_{j}\right|-\lambda\right)_{+}$, where $\hat{\beta}_{j}$ is OLS estimate. 


* The trick here is to standardize $X, y$ before doing computation. Otherwise the $\lambda$ usedin `glmnet` means different things compared to our theoretical computation. I didn't transform the $\hat{\beta}$ back as that simply multplies constants to both solutions which does not affect the comparison. 

* As we can see below, the theoretical computation and the computed value agree pretty well. 
```{r}
X = data_sim$X_train
X = (X - colMeans(X)) / apply(X, 1, sd)
y = data_sim$y_train
y= (y - mean(y)) / sd(y)

## ridge 
cv.ridge = cv.glmnet(X, y,alpha=0, nfolds = 10)
lam = cv.ridge$lambda.min

b.ridge = glmnet(X, y, alpha = 0, lambda = lam)$beta
b.ols = lm(y ~ X)$coefficients[2:101]
b.ridge.t <- b.ols/(1 + lam)

plot(b.ridge, b.ridge.t)
abline(a = 0, b= 1)

## lasso
cv.lasso = cv.glmnet(X, y,alpha=1, nfolds = 10)
lam = cv.lasso$lambda.min
b.ols = lm(y ~ X)$coefficients[2:101]
b.lasso = glmnet(X, y, alpha = 1, lambda = lam)$beta


b.lasso.t <- abs(b.ols) - lam
b.lasso.t[b.lasso.t < 0] = 0
b.lasso.t = b.lasso.t * sign(b.ols)

plot(b.lasso, b.lasso.t)
abline(a = 0, b= 1)
```

### (d)
Check that indeed the sum of absolute values of the coefficients is decreasing along the lasso path. See plot blow
```{r}
plot(log(Y.lasso$lambda), colSums(abs(Y.lasso$beta)), xlab = "log(lambda)", ylab = "sum of abs(beta)", main = "lasso")
```

### (e) 
When you have finished, write a brief summary of what the code is doing, what you examined, and what you learned.

* I learned about how to do CV with `glmnet` and do prediction; understand the shrinkage effects of ridge and lasso regression
* Realized the bias-variance trade-off of k-fold CV; even though they might under-estimate error, they are still very good for choosing the right parameter 
* Learnt the computation trick in `glmnet` and its effect on $\lambda$: do not intrepret $\lambda$ as in the objective function in the text book! This is useful for comparing methods

## ii) 
Note that the simulation in i) involves a non-sparse setting: every predictor has an effect on Y. This might be expected to favor ridge regression over Lasso since ridge regression tends to produce non-sparse solutions, whereas Lasso tends to produce sparse solutions. So now modify the simulation in i) to simulate a sparse scenario, where only 10 of the 100 predictors actually affect 
Y. [Note that you may or may not have to modify the residual variance to make the problem "not too easy" and "not too hard"]. Investigate whether ridge regression or lasso provide better predictions in this setting.

```{r}
simulate_data2 <- function(n = 500, p = 100, p_nz = 10, n_sd = 25, seed = 123){
  set.seed(seed)
  X = matrix(rnorm(n*p),ncol=p)
  b = rnorm(p)
  b[(p_nz+1):p] = 0
  e = rnorm(n,0,sd=n_sd)
  y = X %*% b + e
  return(list(X = X, y = y, b = b))
}

data_sim2 = simulate_data2()

cv.ridge = cv.glmnet(X, y, alpha = 0)
fit.ridge <- glmnet(X, y, alpha = 0, lambda = cv.ridge$lambda)
sum((fit.ridge$beta - data_sim2$b)^2)

cv.lasso = cv.glmnet(X, y, alpha = 1)
fit.lasso <- glmnet(X, y, alpha = 1, lambda = cv.lasso$lambda)
sum((fit.lasso$beta - data_sim2$b)^2)
```

The empirical loss shows that Lasso is btter in  this sparse setting. It is not surprising as the true $\beta$'s are sparse so closer to the assumptions of Lasso regression model than ridge regression model. 

# Problem C

(a) and (b) are hand-written

## (c)
```{r}
nll <- function(par, x, s){
  mu = par[1]
  sigma_sq = par[2]^2
  n_ll = sum( log(sigma_sq + s^2) + (x - mu)^2/(s^2 + sigma_sq) )
  return(n_ll)
}

ebnm_util <- function(x, s, init){
  fit = optim(par = init, fn = nll, x = x, s = s, method = "BFGS", control = list(maxit = 10000))
  mu = fit$par[1]
  sigma_sq = fit$par[2]
  pos.mean = (sigma_sq * x + s^2 * mu)/(sigma_sq + s^2)
  return(list(mu = mu, sigma_sq = sigma_sq, pos.mean = pos.mean, nll = fit$value))
}

init_ebnm <- function(x, s){
  thetahat = x
  mu = mean(thetahat)
  sigma_sq = var(thetahat)
  init = c(mu, sqrt(sigma_sq))
  init = c(1,1)
  return(init)
}

ebnm_ <- function(x, s){
  init = init_ebnm(x = x, s = s)
  return(ebnm_util(x = x, s = s, init = init))
}
```


## (d)
```{r}
simulate_nm <- function(mu, sigma_sq, s, n, seed = 123){
  set.seed(seed)
  theta = rnorm(n = n, mean = mu, sd = sqrt(sigma_sq))
  x = rnorm(n = n, mean = theta, sd = s)
  return(list(theta = theta, x = x, s = s))
}

n = 1000
mu = 0.5
sigma_sq = 2^2
s = replicate(n, 5)

data_nm <- simulate_nm(mu = mu, sigma_sq = sigma_sq, s = s, n = n)
theta = data_nm$theta
x = data_nm$x
s = data_nm$s

fit = ebnm_(x, s)

fit$mu

fit$sigma_sq

par(mfrow = c(2,2))

plot(theta, fit$pos.mean, ylab = "E(theta | X)", 
     main = sprintf("ebnm: loss = %f", mean((theta - fit$pos.mean)^2)))
abline(a = 0, b = 1)

plot(theta, x, ylab = "theta MLE",
     main = sprintf("mle: loss = %f", mean((theta - x)^2)))
abline(a = 0, b = 1)

plot(x, fit$pos.mean, 
     xlab = "mle", ylab = "E(theta | X)", main = "mle vs ebnm")
abline(a = 0, b = 1)


fit2 = ebnm::ebnm_normal(x, s)
loss = list(ebnm_my = mean((theta - fit$pos.mean)^2),
            ebnm_normal = mean((theta - fit2$posterior[,1])^2),
            mle = mean((theta - x)^2))
loss
```

### Comment:
* $\hat{\mu}, \hat{sigma}$ are both close to the true, when $n$ is large (when sample size is small like $n= 100$ we can underestimate $\hat{sigma}$ a lot, but $\hat{\mu}$ is more accurate)
* the posterior mean estimates $\theta$'s much more accurately than MLE estimate, as we can see the (emprical) loss is $3.4 vs 23.4$ 
* Compared with `ebnm::ebnm_normal`, it seems my code gets better loss when $|\mu|$ gets larger whereas `ebnm::ebnm_normal` is better when $\mu$ is closer to $0$ (need more thorough comparisons; seems the default put mode on $0$)

## (e)
```{r}
x = c(28,8,-3,7,-1,1,18,12)
s = c(15,10,16,11,9,11,10,18)
theta = ebnm_(x = x, s = s)$pos.mean
theta
```

We shrinkage all estimates to the same value, similar to what we see in https://stephens999.github.io/stat34800/eight_schools.html . This makes sense as $s$ is very large so there may be no actual differences among schools. 

