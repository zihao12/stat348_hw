---
title: "hw2"
author: "zihao12"
date: "2021-04-10"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---


```{r message=FALSE, warning=FALSE}
rm(list = ls())
library(glmnet)
library(stats)
```


# Problem A

## i) Investigate `glmnet`
```{r}
simulate_data1 <- function(p, n_train, n_test, n_sd = 25, seed = 123){
  set.seed(seed)
  n = n_train + n_test
  X = matrix(rnorm(n*p),ncol=p)
  b = rnorm(p)
  e = rnorm(n,0,sd=n_sd)
  Y = X %*% b + e
  return(list(b = b, 
              X_train = X[1:n_train,], X_test = X[(n_train + 1):n,], 
              y_train = Y[1:n_train], y_test = Y[(n_train + 1):n]))
}
```

### (a) compare CV results with test 
Simulate some independent test data from the same model and check that the prediction error of different methods is comparable with the CV results.
```{r}
data_sim <- simulate_data1(p = 100, n_train = 500, n_test = 500)

```


Ridge regression: 
```{r}
cv.ridge = cv.glmnet(data_sim$X_train,data_sim$y_train,alpha=0, nfolds = 10)
Y.ridge = glmnet(data_sim$X_train,data_sim$y_train,alpha=0)
Yhat_test = predict(Y.ridge, newx = data_sim$X_test, s = cv.ridge$lambda)
err_test = colMeans((Yhat_test - data_sim$y_test)^2)
ylim = c(min(cv.ridge$cvm - cv.ridge$cvsd), max(err_test))
plot(cv.ridge, ylim = ylim, main = "ridge regression prediction error: test vs CV")
lines(log(cv.ridge$lambda), err_test, col = "blue")
```

Lasso regression:
```{r}
cv.lasso = cv.glmnet(data_sim$X_train,data_sim$y_train,alpha=1, nfolds = 10)
Y.lasso = glmnet(data_sim$X_train,data_sim$y_train,alpha=1)
Yhat_test = predict(Y.lasso, newx = data_sim$X_test, s = cv.lasso$lambda)
err_test = colMeans((Yhat_test - data_sim$y_test)^2)
ylim = c(min(cv.lasso$cvm - cv.lasso$cvsd), max(err_test))
plot(cv.lasso, ylim = ylim, main = "lasso regression prediction error: test vs CV")
lines(log(cv.lasso$lambda), err_test, col = "blue")
```


#### Comment:
* The MSE from CV is biased (always underestimated); when setting `nfolds` smaller, the bias gets smaller. 
* However, we can find the appropriate $\lambda$ using CV (that also miniimizes prediction error)

### (b)
Plot the (non-intercept) coefficients obtained from ridge regression and lasso against the true values used in the simulation and discuss the "shrinkage" that is occurring.
```{r}
b.ridge = predict(Y.ridge, type="coefficients", s = cv.ridge$lambda.min)

b.lasso = predict(Y.lasso, type="coefficients", s = cv.lasso$lambda.min)


plot(c(0, data_sim$b), b.ridge, xlab = "b.true")
abline(a = 0, b = 1)

plot(c(0, data_sim$b), b.lasso, xlab = "b.true")
abline(a = 0, b = 1)
```

#### TODO: comment on shrinkage

### (c) TODO!

Plot the estimated (non-intercept) coefficients against the "theoretical" expectations you would expect if the predictors were orthogonal. Eg the "soft thresholding" property for the Lasso. (Note that the predictors here are not orthogonal, so the theory will certainly not hold precisely - does it hold approximately?)

* From the book we know, if $X$ has orthonormal columns, coefficients are ridge are $\hat{\beta}_{j} /(1+\lambda)$ and for lasso are $\operatorname{sign}\left(\hat{\beta}_{j}\right)\left(\left|\hat{\beta}_{j}\right|-\lambda\right)_{+}$, where $\hat{\beta}_{j}$ is OLS estimate. 


* The trick here is to standardize $X, y$ before doing computation. Otherwise the $\lambda$ usedin `glmnet` means different things compared to our theoretical computation. I didn't transform the $\hat{\beta}$ back as that simply multplies constants to both solutions which does not affect the comparison. 

* As we can see below, the theoretical computation 
```{r}
X = data_sim$X_train
X = (X - colMeans(X)) / apply(X, 1, sd)
y = data_sim$y_train
y= (y - mean(y)) / sd(y)

## ridge 
cv.ridge = cv.glmnet(X, y,alpha=0, nfolds = 10)
lam = cv.ridge$lambda.min

b.ridge = glmnet(X, y, alpha = 0, lambda = lam)$beta
b.ols = lm(y ~ X)$coefficients[2:101]
b.ridge.t <- b.ols/(1 + lam)

plot(b.ridge, b.ridge.t)
abline(a = 0, b= 1)

## lasso
cv.lasso = cv.glmnet(X, y,alpha=1, nfolds = 10)
lam = cv.lasso$lambda.min
b.ols = lm(y ~ X)$coefficients[2:101]
b.lasso = glmnet(X, y, alpha = 1, lambda = lam)$beta


b.lasso.t <- abs(b.ols) - lam
b.lasso.t[b.lasso.t < 0] = 0
b.lasso.t = b.lasso.t * sign(b.ols)

plot(b.lasso, b.lasso.t)
abline(a = 0, b= 1)
```

```{r include=FALSE}
beta_ridge <- function(X, y, lam){
  p = ncol(X)
  X_norm = sqrt(colSums(X^2))
  X = X %*% diag(1/X_norm) ## now X has unit L2 norm
  beta = lm(y ~ X)$coefficients[2:(p+1)]
  out = beta / (1 + lam)
  return(out/X_norm)
}

beta_lasso <- function(X,y,lam){
  p = ncol(X)
  X_norm = sqrt(colSums(X^2))
  X = X %*% diag(1/X_norm) ## now X has unit L2 norm
  beta = lm(y ~ X)$coefficients[2:(p+1)]
  out = abs(beta) - lam
  out[out < 0] = 0
  out = out * sign(beta)
  return(out/X_norm)
}


b.ridge.theory <- beta_ridge(X = data_sim$X_train, y = data_sim$y_train, lam = cv.ridge$lambda.min)
#b.lasso.theory <- beta_lasso(beta = b.ols[2:101], lam = cv.lasso$lambda.min)
b.lasso.theory <- beta_lasso(X = data_sim$X_train, y = data_sim$y_train, lam = cv.lasso$lambda.min)



plot(b.ridge[2:101], b.ridge.theory)
abline(a = 0, b = 1)

plot(b.lasso[2:101], b.lasso.theory)
abline(a = 0, b = 1)
```



### (d)
Check that indeed the sum of absolute values of the coefficients is decreasing along the lasso path. See plot blow
```{r}
plot(log(Y.lasso$lambda), colSums(abs(Y.lasso$beta)), xlab = "log(lambda)", ylab = "sum of abs(beta)", main = "lasso")
```

### (e) TODO
When you have finished, write a brief summary of what the code is doing, what you examined, and what you learned. 

## ii) 
Note that the simulation in i) involves a non-sparse setting: every predictor has an effect on Y. This might be expected to favor ridge regression over Lasso since ridge regression tends to produce non-sparse solutions, whereas Lasso tends to produce sparse solutions. So now modify the simulation in i) to simulate a sparse scenario, where only 10 of the 100 predictors actually affect 
Y. [Note that you may or may not have to modify the residual variance to make the problem "not too easy" and "not too hard"]. Investigate whether ridge regression or lasso provide better predictions in this setting.

```{r}
simulate_data2 <- function(n = 500, p = 100, p_nz = 10, n_sd = 25, seed = 123){
  set.seed(seed)
  X = matrix(rnorm(n*p),ncol=p)
  b = rnorm(p)
  b[(p_nz+1):p] = 0
  e = rnorm(n,0,sd=n_sd)
  y = X %*% b + e
  return(list(X = X, y = y, b = b))
}

data_sim2 = simulate_data2()

cv.ridge = cv.glmnet(X, y, alpha = 0)
fit.ridge <- glmnet(X, y, alpha = 0, lambda = cv.ridge$lambda)
sum((fit.ridge$beta - data_sim2$b)^2)

cv.lasso = cv.glmnet(X, y, alpha = 1)
fit.lasso <- glmnet(X, y, alpha = 1, lambda = cv.lasso$lambda)
sum((fit.lasso$beta - data_sim2$b)^2)
```

The empirical loss shows that Lasso is btter in  this sparse setting. 

# Problem C

(a) and (b) are hand-written

## (c)
```{r}
nll <- function(par, x, s){
  mu = par[1]
  sigma_sq = par[2]^2
  n_ll = sum( log(sigma_sq + s^2) + (x - mu)^2/(s^2 + sigma_sq) )
  return(n_ll)
}

ebnm_util <- function(x, s, init){
  fit = optim(par = init, fn = nll, x = x, s = s, method = "BFGS", control = list(maxit = 10000))
  mu = fit$par[1]
  sigma_sq = fit$par[2]
  pos.mean = (sigma_sq * x + s^2 * mu)/(sigma_sq + s^2)
  return(list(mu = mu, sigma_sq = sigma_sq, pos.mean = pos.mean, nll = fit$value))
}

init_ebnm <- function(x, s){
  thetahat = x
  mu = mean(thetahat)
  sigma_sq = var(thetahat)
  init = c(mu, sqrt(sigma_sq))
  init = c(1,1)
  return(init)
}

ebnm_ <- function(x, s){
  init = init_ebnm(x = x, s = s)
  return(ebnm_util(x = x, s = s, init = init))
}
```


## (d)
```{r}
simulate_nm <- function(mu, sigma_sq, s, n, seed = 123){
  set.seed(seed)
  theta = rnorm(n = n, mean = mu, sd = sqrt(sigma_sq))
  x = rnorm(n = n, mean = theta, sd = s)
  return(list(theta = theta, x = x, s = s))
}

n = 1000
mu = 0.5
sigma_sq = 2^2
s = replicate(n, 5)

data_nm <- simulate_nm(mu = mu, sigma_sq = sigma_sq, s = s, n = n)
theta = data_nm$theta
x = data_nm$x
s = data_nm$s

fit = ebnm_(x, s)

fit$mu

fit$sigma_sq

par(mfrow = c(2,2))

plot(theta, fit$pos.mean, ylab = "E(theta | X)", 
     main = sprintf("ebnm: loss = %f", mean((theta - fit$pos.mean)^2)))
abline(a = 0, b = 1)

plot(theta, x, ylab = "theta MLE",
     main = sprintf("mle: loss = %f", mean((theta - x)^2)))
abline(a = 0, b = 1)

plot(x, fit$pos.mean, 
     xlab = "mle", ylab = "E(theta | X)", main = "mle vs ebnm")
abline(a = 0, b = 1)


fit2 = ebnm::ebnm_normal(x, s)
loss = list(ebnm_my = mean((theta - fit$pos.mean)^2),
            ebnm_normal = mean((theta - fit2$posterior[,1])^2),
            mle = mean((theta - x)^2))
loss
```

### Comment:
* $\hat{\mu}, \hat{sigma}$ are both close to the true (when sample size is small like $n= 100$ we can underestimate $\hat{sigma}$ a lot; but when $n$ gets large the estimates are very good )
* the posterior mean estimates $\theta$'s much more accurately than MLE estimate, as we can see the (emprical) loss is $3.4 vs 23.4$ 
* Compared with `ebnm::ebnm_normal`, it seems my code gets better loss when $|\mu|$ gets larger whereas `ebnm::ebnm_normal` is better when $\mu$ is closer to $0$ (need more thorough comparisons; seems the default put mode on $0$)

## (e)
```{r}
x = c(28,8,-3,7,-1,1,18,12)
s = c(15,10,16,11,9,11,10,18)
theta = ebnm_(x = x, s = s)$pos.mean
theta
```

We shrinkage all estimates to the same value, similar to what we see in https://stephens999.github.io/stat34800/eight_schools.html . This makes sense as $s$ is very large so there may be no actual differences among schools. 

