---
title: "hw1"
author: "zihao12"
date: "2021-04-04"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r message=FALSE, warning=FALSE}
rm(list = ls())
set.seed(123)
library(imager)
library(class)
```


## Problem A
```{r}
simulate_tusk <- function(f, n = 1000){
  k = length(f)
  return(t(matrix(rbinom(n = n*k, size = 1, prob = f), nrow = k)))
}

simulate_p1 <- function(fS, fF, nF,nS){
  forest <- simulate_tusk(f = fF, n = nF)
  savan <- simulate_tusk(f = fS,n = nS)
  X = rbind(forest, savan)
  y = c(replicate(nF, 0), replicate(nS,1)) ## forest is 0, savana is 1
  return(list(X = X, y = y))
}

classifier_lr <- function(data, fS, fF){
  X = data$X 
  y = data$y
  ## compute loglikelihood for each sample, for model S & F respectively
  llS <-  rowSums(X %*% diag(log(fS)) + (1 - X) %*% diag(log(1-fS)))
  llF <-  rowSums(X %*% diag(log(fF)) + (1 - X) %*% diag(log(1-fF)))
  ## compute LR for each sample
  LR = exp(llS - llF)
  return(LR)  
}

experiment_p1 <- function(fS, fF, nF,nS, cs, seed = 123){
  set.seed(seed)
  data <- simulate_p1(fS, fF, nF,nS)
  LR <- classifier_lr(data, fS, fF)
  err_rates <- c()
  for(c in cs){
    yhat <- (LR > c)
    err_rate <- mean(yhat != data$y)
    err_rates <- c(err_rates, err_rate)
  }
  return(err_rates)
}
```

### (1)
```{r}
fS = c(0.40, 0.12,0.21,0.12,0.02,0.32)
fF = c(0.8,0.2,0.11,0.17,0.23,0.25)
log10_cs = seq(-2, 2, by = 0.001)

err_rate = experiment_p1(fS = fS, fF = fF, nF = 1000, nS = 1000,
              cs = 10^log10_cs)

plot(log10_cs, err_rate, type = "l",
     xlab = "log10(c)",ylab = "err_rate", main = "log10(c) vs error rate")

c_min = 10^(log10_cs[which.min(err_rate)])
print(sprintf("minimizer of error rate is c = %f, with error rate =  %f", c_min, min(err_rate)))
```

### (2)
```{r}
log10_cs = seq(-1, 2, by = 0.001)

err_rate = experiment_p1(fS = fS, fF = fF, nF = 1900, nS = 100,
              cs = 10^log10_cs)

plot(log10_cs, err_rate, type = "l",
     xlab = "log10(c)",ylab = "err_rate", main = "log10(c) vs error rate")

c_min = 10^(log10_cs[which.min(err_rate)])
print(sprintf("minimizer of error rate is c = %f, with error rate =  %f", c_min, min(err_rate)))
```

* The minimizer is $c = 7.709035$, the largest LR (i.e. classifiying every sample as Forest). Why? Because the Forest has way more samples, and that the LRs for Savana's LRs are not separable from that of Forest.  

* Specifically, if we want to correctly classify any Savana sample (i.e. set a larger $c$), we would sacrifice more mistakes among Forest samples, as can be seen clearly from the histogram below. 

* Therefore to make the least mistakes, the best decision rule is to classify everything as Forest. 

```{r}
data  <- simulate_p1(fS, fF, nF = 1900, nS = 100)
LR = classifier_lr(data, fS, fF)
hF <- hist(LR[1:1900],plot = FALSE,breaks = 30)
hS <- hist(LR[1901:2000],plot = FALSE, breaks = 30)
plot(hF, col = "red",xlab = "LR")
plot(hS, col = "blue",add = TRUE)
legend("topright",legend = c("Forest", "Savana"), 
       col = c("red", "blue"), lty=1:1)
```


## Problem B

### (1) Load data and visualize
```{r}
load_data <- function(filename){
  data = read.table(filename, header = FALSE)
  data <- data.frame(data)
  colnames(data) <- c("y", 1:256)
  return(data)
}

subset_23 <- function(data){
  idx <- which(data$y %in% c(2,3))
  data <- data[idx, ]
  data$y <- (data$y == 2)
  return(data)
}

plot_digit <- function(data, idx){
  im <- matrix(as.numeric(data[idx,2:257]), nrow = 16, ncol = 16)
  image(t(apply(-im,1,rev)),col=gray((0:32)/32), main = data$y[idx])
}
```


```{r}
data_test = load_data("data/zip.test")
data_train = load_data("data/zip.train")

par(mfrow = c(2,2))
ids <- c(1,12,34,77)
for(i in ids){
  plot_digit(data_train, i)
}
```


### (2) classfier for digits 2 & 3
get the subset of data with labels 2 or 3
```{r}
data_train23 = subset_23(data_train)
data_test23 = subset_23(data_test)
```

#### fit with Logistic regression
```{r}
compute_loss <- function(y,yhat, cost){
  diff <- y - yhat
  loss = cost[1] * sum(diff == 1) + cost[2] * sum(diff == -1)
  return(loss)
}

## use fitted glm model and cost to predict yhat, and compute loss
pred_logistic <- function(fit, cost, new_data, y){
  p0 = cost[2]/sum(cost)
  phat <- predict.glm(fit, newdata = new_data, type = "response")
  yhat <- (phat > p0)
  loss <- compute_loss(y, yhat, cost)
  return(list(yhat = yhat, loss = loss))
}

## cost[1] is for (y, yhat) = (1, 0)
## cost[2] is for (y, yhat) = (0, 1)
classifier_logistic <- function(data_train, data_test, cost = c(1,1)){
  fit <- glm(y ~ ., family=binomial(link='logit'), data = data_train)
  pred_train <- pred_logistic(fit = fit, cost = cost, 
                              new_data = data_train[,2:257], y = data_train$y)
  pred_test <- pred_logistic(fit = fit, cost = cost, 
                              new_data = data_test[,2:257], y = data_test$y)
  return(list(loss_test = pred_test$loss, loss_train = pred_train$loss, 
              yhat_test = pred_test$yhat, yhat_train = pred_train$yhat, fit = fit))
}

outcome_logistic = classifier_logistic(data_train23, data_test23)
```

* Note: by the numerical 0/1 and the 0 error rate, we know the 2 & 3's are perfectly separable. 

#### fit with K-NN
```{r}
## given knn output (with prob = T), make prediction based on cutoff probability
pred_knn <- function(fit, y, cost){
  p0 = cost[2]/sum(cost)
  winning = as.numeric(fit) - 1 ## the prediction is 1 & 2,which should be 0 & 1
  phat = attr(fit,"prob") ## its prob for the winning class
  phat = winning * phat + (1-winning) * (1 - phat)
  yhat  <- (phat > p0)
  loss <- compute_loss(y, yhat, cost)
  return(list(loss =loss, yhat = yhat))
}

classifier_knn <- function(data_train, data_test, k, cost = c(1,1)){
  p0 = cost[2]/sum(cost)
  ## fit and predict on test data
  fit_test <- class::knn(data_train[, 2:257], data_test[,2:257], data_train[,1], k = k, prob = TRUE) 
  pred_test <- pred_knn(fit = fit_test, y = data_test$y, cost= cost)
  
  ## do the same for training data
  fit_train <- class::knn(data_train[, 2:257], data_train[, 2:257], data_train[,1], k = k, prob = TRUE)
  pred_train <- pred_knn(fit = fit_train, y = data_train$y, cost= cost)
  return(list(loss_test = pred_test$loss, loss_train = pred_train$loss, 
              yhat_test = pred_test$yhat, yhat_train = pred_train$yhat, 
              fit_test = fit_test, fit_train = fit_train))
}

ks <- c(NA, 1,3,5,7,15)
models <- c("logistic", replicate(5, "KNN"))
err_train <- c(outcome_logistic$loss_train/nrow(data_train23))
err_test <- c(outcome_logistic$loss_test/nrow(data_test23))

for(i in 2:length(ks)){
  out <- classifier_knn(data_train23, data_test23, ks[i])
  err_train <- c(err_train, out$loss_train/nrow(data_train23))
  err_test <- c(err_test, out$loss_test/nrow(data_test23))
}
```

#### summarize results
In the plot below red and blue dots are KNN test err and training err; the upper line is test error for glm and the bottom line (which is $0$) is training error for glm.   
```{r}
data.frame(model = models, k = ks, err_train = err_train, err_test = err_test)

plot(ks[2:length(ks)], err_test[2:length(ks)], col = "red", xlab = "k", ylab = "err rate", ylim = c(0, 0.06))
points(ks[2:length(ks)], err_train[2:length(ks)], col = "blue")
abline(h = err_train[1], lty = 3, col = "red") 
abline(h = err_test[1], lty = 3, col = "red") 
```

### (3) use CV to choose k for KNN
* I will use 10-fold cross-validation
* In practice I think it is better to use $k$ from $1$ to large number say $50$ consecutively (I tried and find $k = 5$ is the best). But below I use $k=1,3,5,7,15$ to better compare with the results from the previous question. 
```{r}
knn_cv <- function(data_train, k, d = 10, cost = c(1,1)){
  N = nrow(data_train)
  grp_size = floor(N/d)
  loss <- 0
  for(i in 1:d){
    start = grp_size*(i-1)
    end = ifelse(i < d, grp_size*i, d)
    idx = start:end
    fit <- class::knn(data_train[-idx, 2:257], data_train[idx, 2:257], data_train[-idx,1], k = k, prob = TRUE)
    loss = loss +  
      length(data_train$y[idx]) * pred_knn(fit = fit, y = data_train$y[idx], cost = cost)$loss
  }
  return(loss/N)
}

err_cv <- c()
ks <- c(1,3,5,7,15)
for(k in ks){
  err_cv <- c(err_cv, knn_cv(data_train23, k))
}
plot(ks, err_cv)
```

* From the CV result above, $k = 3$ is the best choice (tried 10-fold CV and also have $K = 3$ is the best). 

* It's not in agreement with the test data, but not off by much. 

### (4) modify classifier according to loss

Compare the expected loss for the two actions under the new loss (below $p$ is the probability for being 2, estimated by $\hat{p}$ from the model):
\begin{align}
& E(L(y,\hat{y} = 2)) = (1 - p) * \text{cost of misclassifying 3 as 2} = (1-p)\\
& E(L(y,\hat{y} = 3)) = p * \text{cost of misclassifying 2 as 3} = 5p
\end{align}
Therefore we predict $\hat{y} = 2$ when $\hat{p} \geq \frac{1}{6}$. 

For both logistic regression and KNN I already incorporated cost in the functions above. 

#### logistic regression
```{r}
loss_new = classifier_logistic(data_train23, data_test23, cost = c(5, 1))$loss_test

loss_old = compute_loss(y = data_test23[,1], yhat = outcome_logistic$yhat_test, cost = c(5,1))

print(sprintf("test loss new vs old: %f vs %f", loss_new, loss_old))
```

#### KNN
I compared a couple of KNN's below
```{r}
compare_knn_new_old <- function(k){
  print(sprintf("%d-KNN:", k))
  fit <- class::knn(data_train23[, 2:257], data_test23[,2:257], data_train23[,1], k = k, prob = TRUE) 
  pred_old <- pred_knn(fit = fit, y = data_test23[,1], cost = c(1,1))
  pred_new <- pred_knn(fit = fit, y = data_test23[,1], cost = c(5,1))
  
  loss_old = compute_loss(y = data_test23[,1], yhat = pred_old$yhat, cost = c(5,1))
  loss_new = pred_new$loss
  
  print(sprintf("test loss new vs old: %f vs %f", loss_new, loss_old))
}

for(k in ks){
  compare_knn_new_old(k)
}
```
Interesting that 1-KNN the cost does not matter: it turns out that the predicted $\hat{y}$ are the same for the two models. The test data is too easily separable (the prediction probability is very confident) for 1-KNN that even such an asymmetric cost does not matter much. 

## Problem C  

### Load and prepare data (from https://github.com/stephens999/stat302/blob/master/exercises/seeb/train_test.R). 
```{r}
data_preprocess <- function(){
  orig_data = read.table("data/four_salmon_pops.csv",header=TRUE,colClasses="character",sep=",")
  set.seed(100) #to ensure reproducibility
  
  #Convert the data at each locus to a factor
  #Note that we have to be careful to include all the levels from *both* columns
  #for each locus
  mylevels= function(locus){levels(factor(c(orig_data[,(1+2*locus)],
                                            orig_data[,(2+2*locus)])))}
  
  #now set up four_salmon_pops
  four_salmon_pops = orig_data
  for(locus in 1:12){
    four_salmon_pops[,1+2*locus]= factor(four_salmon_pops[,1+2*locus],levels = mylevels(locus))
    four_salmon_pops[,2+2*locus]= factor(four_salmon_pops[,2+2*locus],levels = mylevels(locus))
  }
  
  #Randomly divide the data into a training set and a test set
  nsamp = nrow(four_salmon_pops)
  subset = (rbinom(nsamp,1,0.5)==1) #include each fish in subset with probability 0.5
  train = four_salmon_pops[subset,]
  test = four_salmon_pops[!subset,]
  return(list(train = train, test = test, whole = four_salmon_pops))
}

data  = data_preprocess()
train = data$train
test = data$test
whole = data$whole
```

### Modeling
* Data representation: for each sample,let's convert the data into count table. Say a sample $i$ and locus $l$ is a vector $x_{il}$ of length $d_l$ (number of possible alleles at locus $l$). For example, if it has alleles $a_1$, $a_2$, $x_{ilj} = 1$ when $j = a_1, a_2$ and $x_{ilj} = 1$ otherwise. 

* Hidden variable: introduce $z_{i}$ to denote where population sample $i$ belongs. 

* Likelihood: Then we can model $x_{il}$ with a multinomial distribution: $x_{il} | z_{i} = p \sim MN(2, \pi_{pl})$ where $\pi_{pl}$ is the proportion of alleles in locus $l$ and population $p$. Then we can write the likelihood as follows

\begin{align}
p(x_i | z_i= p) & = \Pi_{l = 1}^{12} p(x_{il} | z_i = p)\\
& = \Pi_{l = 1}^{12} MN(x_{il}; 2, \pi_{pl})
\end{align}

* Prior: since all four populations are equaly likeliy, then $p(z_i = p) = 1/4$. 

* Posterior:

\begin{align}
p(z_i = p | x_i) & \propto p(x_i | z_i = p) p(z_i= p)\\
& \propto p(x_i | z_i = p)
\end{align}

Then since $\sum_{p = 1}^4 p(z_i = p | x_i) = 1$,  we have
$$  p(z_i = p | x_i) = \frac{p(x_i | z_i = p)}{\sum_{p^{'}} p(x_i | z_i = p^{'})}  $$

```{r}
#this function computes a table of the alleles and their counts at a given locus (locus= 1...12)
#in a given data frame (data)
compute_counts = function(data,locus){
  return(table(data[,1+2*locus]) + table(data[,2+2*locus]))
}
## this function converts counts to frequency (proportions)
normalize= function(x){x/sum(x)}
```


### compute frequency from training data
* Note I didn't consider NA in the computig frequency (will be discussed in the end) 
* I added small $\epsilon$ to those $0$ probabilities. $0$ may not be good estimate (indeed they appear in test data) and can cause numerical issues. 
```{r}
pop = unique(train$Population)
nloc = 12

freqs <- list()
for(i in 1:length(pop)){
  data_sub = train[train$Population == pop[i], ]
  f <- list()
  for(l in 1:nloc){
    f_ = normalize(compute_counts(data_sub, l))
    f_[f_ == 0] = 1e-8
    f[[l]] = normalize(f_)
  }
  freqs[[i]] = f
}
```

### compute posterior
* The code is not optimized (uses many for loops) but is okay for this small problem...
```{r}
prob_pos <- matrix(NA, nrow = nrow(test), ncol = length(pop))

for(i in 1:nrow(test)){
  lls <- replicate(length(pop), NA) ## store p(x_i | z_i = p) for each p
  for(p in 1:length(pop)){
    ll = 0
    for(l in 1:nloc){
       counts = compute_counts(test[i,], l)
       if(sum(counts) == 2){ ## no missing
         ll = ll + dmultinom(x = counts, size = 2, prob = freqs[[p]][[l]], log = TRUE)
         if(is.nan(ll)){browser()}
       }
    }
    lls[p] = ll
  }
  prob = exp(lls)/sum(exp(lls))
  prob_pos[i,] <- prob 
}
```

### prediction
The error rate is reported below
```{r}
z_pred <- apply(prob_pos, 1, which.max)
err_rate = sum(pop[z_pred] != test$Population)/nrow(test)
err_rate
```

### Discussions:
There are two challenges: `NA` in the data and problems caused by exact $0$ in allele frequency

Problems from `NA`

* There are `NA`'s in data, which can affect both estimated allele frequency from training data, and the likelihood for test data. 
* I didn't specifically model `NA`'s, except that checking if each $x_{il}$ sums to exactly 2 (otherwise it cannot be fitted by my model).
* However, with more background knowledge on the measurement, it might be the case that the data is not missing at random, where my model leads to biased estimate. 
  
Problem from allele frequency being exact 0's:

* Estimate of allele frequency gives some exact 0's, which is probably not true and causes numerical issues in test data (they appear in test data!)
* I added some constant $10^{-8}$ to those $0$'s and re-normalize the probability vectors. 
