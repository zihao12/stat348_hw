---
title: "hw1"
author: "zihao12"
date: "2021-04-04"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r message=FALSE, warning=FALSE}
rm(list = ls())
set.seed(123)
library(imager)
library(class)
```


## Problem A

```{r}
simulate_tusk <- function(f, n = 1000){
  k = length(f)
  return(t(matrix(rbinom(n = n*k, size = 1, prob = f), nrow = k)))
}

simulate_p1 <- function(fS, fF, nF,nS){
  forest <- simulate_tusk(f = fF, n = nF)
  savan <- simulate_tusk(f = fS,n = nS)
  X = rbind(forest, savan)
  y = c(replicate(nF, 1), replicate(nS,0)) ## forest is 1, savana is 0
  return(list(X = X, y = y))
}

classifier_lr <- function(data, fS, fF){
  X = data$X 
  y = data$y
  ## compute loglikelihood for each sample, for model S & F respectively
  llS <-  rowSums(X %*% diag(log(fS)) + (1 - X) %*% diag(log(1-fS)))
  llF <-  rowSums(X %*% diag(log(fF)) + (1 - X) %*% diag(log(1-fF)))
  ## compute LR for each sample
  LR = exp(llF - llS)
  return(LR)  
}

experiment_p1 <- function(fS, fF, nF,nS, cs, seed = 123){
  set.seed(seed)
  data <- simulate_p1(fS, fF, nF,nS)
  LR <- classifier_lr(data, fS, fF)
  err_rates <- c()
  for(c in cs){
    yhat <- (LR > c)
    err_rate <- mean(yhat != data$y)
    err_rates <- c(err_rates, err_rate)
  }
  return(err_rates)
}

```

### (1)
```{r}
fS = c(0.40, 0.12,0.21,0.12,0.02,0.32)
fF = c(0.8,0.2,0.11,0.17,0.23,0.25)
log10_cs = seq(-2, 2, by = 0.001)

err_rate = experiment_p1(fS = fS, fF = fF, nF = 1000, nS = 1000,
              cs = 10^log10_cs)

plot(log10_cs, err_rate, type = "l",
     xlab = "log10(c)",ylab = "err_rate", main = "log10(c) vs error rate")

c_min = 10^(log10_cs[which.min(err_rate)])
print(sprintf("minimizer of error rate is c = %f, with error rate =  %f", c_min, min(err_rate)))
```

### (2)
```{r}
log10_cs = seq(-6, 0, by = 0.001)

err_rate = experiment_p1(fS = fS, fF = fF, nF = 1900, nS = 100,
              cs = 10^log10_cs)

plot(log10_cs, err_rate, type = "l",
     xlab = "log10(c)",ylab = "err_rate", main = "log10(c) vs error rate")

c_min = 10^(log10_cs[which.min(err_rate)])
print(sprintf("minimizer of error rate is c = %f, with error rate =  %f", c_min, min(err_rate)))
```

* The minimizer is $c = 0$ (i.e. classifiying every sample as Forest). Why? Because the Forest has way more samples, and that the LRs for Savana's LRs are not separable from that of Forest.  

* Specifically, if we want to correctly classify any Savana sample (i.e. set a larger $c$), we would sacrifice more mistakes among Forest samples, as can be seen clearly from the histogram below. 

* Therefore to make the least mistakes, the best decision rule is to classify everything as Forest. 

```{r}
data  <- simulate_p1(fS, fF, nF = 1900, nS = 100)
LR = classifier_lr(data, fS, fF)
hF <- hist(LR[1:1900],plot = FALSE,breaks = 30)
hS <- hist(LR[1901:2000],plot = FALSE, breaks = 30)
plot(hF, col = "red",xlab = "LR")
plot(hS, col = "blue",add = TRUE)
legend("topright",legend = c("Forest", "Savana"), 
       col = c("red", "blue"), lty=1:1)
```


## Problem 2

### (1) Load data and visualize
```{r}
load_data <- function(filename){
  data = read.table(filename, header = FALSE)
  data <- data.frame(data)
  colnames(data) <- c("y", 1:256)
  return(data)
}

subset_23 <- function(data){
  idx <- which(data$y %in% c(2,3))
  data <- data[idx, ]
  data$y <- (data$y == 2)
  return(data)
}

plot_digit <- function(data, idx){
  im <- matrix(as.numeric(data[idx,2:257]), nrow = 16, ncol = 16)
  image(t(apply(-im,1,rev)),col=gray((0:32)/32), main = data$y[idx])
}
```


```{r}
data_test = load_data("data/zip.test")
data_train = load_data("data/zip.train")

par(mfrow = c(2,2))
ids <- c(1,12,34,77)
for(i in ids){
  plot_digit(data_train, i)
}
```


### (2) classfier for digits 2 & 3
get the subset of data with labels 2 or 3
```{r}
data_train23 = subset_23(data_train)
data_test23 = subset_23(data_test)
```

#### fit with Logistic regression
```{r}
compute_loss <- function(y,yhat, cost){
  diff <- y - yhat
  loss = cost[1] * sum(diff == 1) + cost[2] * sum(diff == -1)
  return(loss/length(y))
}


## cost[1] is for (y, yhat) = (1, 0)
## cost[2] is for (y, yhat) = (0, 1)
classifier_logistic <- function(data_train, data_test, cost = c(1,1)){
  fit <- glm(y ~ ., family=binomial(link='logit'), data = data_train)
  phat_test <- predict.glm(fit, newdata = data_test[,2:257], type = "response")
  phat_train <- predict.glm(fit, newdata = data_train[,2:257], type = "response")
  p0 = cost[1]/sum(cost)
  yhat_train <- (phat_train > p0)
  # err_train = mean(yhat_train != data_train$y)
  loss_train <- compute_loss(data_train$y, yhat_train, cost)
  yhat_test <- (phat_test > p0)
  # err_test = mean(yhat_test != data_test$y)
  loss_test <- compute_loss(data_test$y, yhat_test, cost)
  return(list(loss_test = loss_test, loss_train = loss_train))
}

outcome_logistic = classifier_logistic(data_train23, data_test23)
```

* Note: by the numerical 0/1 and the 0 error rate, we know the 2 & 3's are perfectly separable. 

#### fit with K-NN
```{r}
## given knn output (with prob = T), make prediction based on cutoff probability
knn_pred <- function(pred, p0){
  winning = as.numeric(pred) - 1 ## the prediction is 1 & 2,which should be 0 & 1
  phat = attr(pred,"prob") ## its prob for the winning class
  phat = winning * phat + (1-winning) * (1 - phat)
  yhat  <- (phat > p0)
  return(yhat)
}


classifier_knn <- function(data_train, data_test, k, cost = c(1,1)){
  p0 = cost[1]/sum(cost)
  ## fit and predict on test data
  pred_test <- class::knn(data_train[, 2:257], data_test[,2:257], data_train[,1], k = k, prob = TRUE) 
  yhat_test <- knn_pred(pred_test, p0)
  loss_test <- compute_loss(data_test$y, yhat_test, cost)
  
  ## do the same for training data
  pred_train <- class::knn(data_train[, 2:257], data_train[, 2:257], data_train[,1], k = k, prob = TRUE)
  yhat_train <- knn_pred(pred_train, p0)
  loss_train <- compute_loss(data_train$y, yhat_train, cost)
  return(list(loss_test = loss_test, loss_train = loss_train))
}

ks <- c(NA, 1,3,5,7,15)
models <- c("logistic", replicate(5, "KNN"))
err_train <- c(outcome_logistic$loss_train)
err_test <- c(outcome_logistic$loss_test)

for(i in 2:length(ks)){
  out <- classifier_knn(data_train23, data_test23, ks[i])
  err_train <- c(err_train, out$loss_train)
  err_test <- c(err_test, out$loss_test)
}
```

#### summarize results
TODO: plot what?
```{r}
data.frame(model = models, k = ks, err_train = err_train, err_test = err_test)
```

### (3) use CV to choose k for KNN
I will use 10-fold cross validation
```{r}
knn_cv <- function(data_train, k, d = 10){
  N = nrow(data_train)
  grp_size = floor(N/d)
  errs <- 0
  for(i in 1:d){
    start = grp_size*(i-1)
    end = ifelse(i < d, grp_size*i, d)
    idx = start:end
    yhat_test <- class::knn(data_train[-idx, 2:257], data_train[idx, 2:257], data_train[-idx,1], k = k)
    errs = errs +  sum( yhat_test != data_train$y[idx])
  }
  return(errs/N)
}

err_cv <- c()
ks <- c(1,3,5,7,15)
for(k in ks){
  err_cv <- c(err_cv, knn_cv(data_train23, k))
}
err_cv
```

From the CV result above, $k = 3$ is the best choice. BUT not in agreement with the test data...

### (4) modify classifier according to loss

Compare the expected loss for the two actions under the new loss (below $p$ is the probability for being 2, estimated by $\hat{p}$ from the model):
\begin{align}
& E(L(y,\hat{y} = 2)) = (1 - p) * \text{cost of misclassifying 3 as 2} = (1-p)\\
& & E(L(y,\hat{y} = 3)) = p * \text{cost of misclassifying 2 as 3} = 5p
\end{align}
Therefore we predict $\hat{y] = 2$ when $\hat{p} \geq \frac{1}{6}$

## Problem C  
